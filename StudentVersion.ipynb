{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP2- Traffic Sign Classifier\n",
    "Before you start, please read through this tutorial of Pytorch: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py.\n",
    "The basic structure of this MP will be very similar to the example code in the tutorial. But your neural network model need to be more complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any packege is not installed, you shall be able to install them through pip3 install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the parameters. If you have GPU, change the num_gpu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f039c3bafb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of workers for dataloader\n",
    "workers = 4\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. The image size of original images vary from 15 to 250. \n",
    "#But to train our CNN we must have the fixed size for the inputs.\n",
    "#All images will be resized to this size using a transformer.\n",
    "image_size = 32\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 15\n",
    "\n",
    "#number of training classes\n",
    "num_classes = 43\n",
    "\n",
    "# Learning rate for optimizers\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "num_gpu = 1\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "To improve the performance of our CNN, we need to use some data augmentation techniques. We can change the orientation, location, scale, saturation or brightness of the original image to modify the original images or create new training images. \n",
    "\n",
    "The easiest way to go is openCV. If you are having problem with ppm format, you can convert the images to other image format (PNG, JPG...) using pillow packege: https://pillow.readthedocs.io/en/latest/handbook/tutorial.html\n",
    "\n",
    "You are reqiured to test at least 3 techniques and compare the effectiveness(the accuracy improvement). For each technique, you need to generate new images for at least one class of images. For instance, if you originally have 300 training images in class 01, then you need to generate 300 new images and save them into the same folder as old images. Note that it may be helpful to name the new images properly, so if you find your technique is not helpful, you can remove them easier. <br>\n",
    "You can skip this part and try to have a working neural network first, then come back here to finish it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "def transform(img_dir, method):\n",
    "\tfor subdir, dirs, imgs in os.walk(img_dir):\n",
    "\t\tfor img_name in imgs:\n",
    "\t\t\tif img_name.endswith('.ppm'):\n",
    "\t\t\t\timg = cv2.imread(os.path.join(img_dir, img_name), 1)\n",
    "\t\t\t\t# scaling images\n",
    "\t\t\t\tif method == 'scale':\n",
    "\t\t\t\t\tchoices = [0, 1]\n",
    "\t\t\t\t\tchoice = random.choice(choices)\n",
    "\t\t\t\t\t# enlarge or shrink the image by factor of 2\n",
    "\t\t\t\t\trows, cols, channel = img.shape\n",
    "\t\t\t\t\t# print(rows, cols)\n",
    "\t\t\t\t\tif choice == 0:\n",
    "\t\t\t\t\t\tres = cv2.resize(img, (rows*2, cols*2), interpolation = cv2.INTER_CUBIC)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tres = cv2.resize(img, (int(rows/2), int(cols/2)), interpolation = cv2.INTER_AREA)\n",
    "\t\t\t\t\n",
    "\t\t\t\telif method == 'rotate':\n",
    "\t\t\t\t\tchoices = [0, 1, 2]\n",
    "\t\t\t\t\tchoice = random.choice(choices)\n",
    "\t\t\t\t\trows, cols, channel = img.shape\n",
    "\t\t\t\t\tif choice == 0:\n",
    "\t\t\t\t\t\tM = cv2.getRotationMatrix2D((cols/2, rows/2), 90, 1)\n",
    "\t\t\t\t\telif choice == 1:\n",
    "\t\t\t\t\t\tM = cv2.getRotationMatrix2D((cols/2, rows/2), 180, 1)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tM = cv2.getRotationMatrix2D((cols/2, rows/2), 270, 1)\n",
    "\t\t\t\t\tres = cv2.warpAffine(img, M, (cols, rows))\n",
    "\t\t\t\t\n",
    "\t\t\t\telif method == 'flip':\n",
    "\t\t\t\t\tchoices = [0, 1, 2]\n",
    "\t\t\t\t\tchoice = random.choice(choices)\n",
    "\t\t\t\t\tif choice == 0:\n",
    "\t\t\t\t\t\tres = cv2.flip(img, 0)\n",
    "\t\t\t\t\telif choice == 1:\n",
    "\t\t\t\t\t\tres = cv2.flip(img, 1)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tres = cv2.flip(img, -1)\n",
    "\t\n",
    "\t\t\t\tname, extension = os.path.split(img_name)\n",
    "\t\t\t\tres_name = name + 'trans' + extension\n",
    "\t\t\t\tcv2.imwrite(os.path.join(img_dir, res_name), res)\n",
    "\t\t\t\t\n",
    "scale_dir = '/home/peixin/mp2_data/Final_Training_scale/Images/00024/'\n",
    "rotate_dir = '/home/peixin/mp2_data/Final_Training_rotate/Images/00024/'\n",
    "flip_dir = '/home/peixin/mp2_data/Final_Training_flip/Images/00024/'\n",
    "\n",
    "# transform(scale_dir, 'scale')\n",
    "transform(rotate_dir, 'rotate')\n",
    "transform(flip_dir, 'flip')\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Load the dataset into PyTorch. Assign the path to the training and testing dataset you downloaded previously to the variables \"trainingClassifierRoot\" and \"testClassifierRoot\" below. In the beginning you can just use the original data downloaded from website. Later on you will need to process the image before load them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "##TODO\n",
    " #trainingClassifierRoot = '/home/peixin/mp2_data/Final_Training/Images'\n",
    "trainingClassifierRoot = '/home/peixin/mp2_data/Final_Training_flip/Images'\n",
    "testClassifierRoot = '/home/peixin/mp2_data/Online-Test-sort/'\n",
    "####\n",
    "normImgTensor = transforms.Normalize(mean=np.zeros(3), std=np.ones(3))\n",
    "\n",
    "# Create the dataset\n",
    "trainClassifierDataset = dset.ImageFolder(root=trainingClassifierRoot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\t\t\t\t\t\t\t   normImgTensor\n",
    "                           ]))\n",
    "\n",
    "testClassifierDataset = dset.ImageFolder(root=testClassifierRoot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\t\t\t\t\t\t\t   normImgTensor\n",
    "                           ]))\n",
    "\n",
    "# Create the dataloader\n",
    "trainClassifierLoader = torch.utils.data.DataLoader(trainClassifierDataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "testClassifierLoader = torch.utils.data.DataLoader(testClassifierDataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device(GPU or CPU) we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and num_gpu > 0) else \"cpu\")\n",
    "\n",
    "print ('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the model\n",
    "In this part you will design the structure of CNN. The basic syntax is the same as the toy example in PyTorch tutorial. But you definitely need some more complex models to meet the reqiurements. You need to try at least 3 different structures and report their accuracy and running time (how long it would take to go through the test set). The best one is required to achieve 97% accuracy. Describe the CNN structures and compare the results in your report. Feel free to use any existing structure you find on Internet, but citation in your report is needed.<br>\n",
    "Here is a NN structure that has 98% accuracy. You can start from there: https://chatbotslife.com/german-sign-classification-using-deep-learning-neural-networks-98-8-solution-d05656bf51ad<br>\n",
    "If you really want to get higher accuracy, here are some state-of-the-art papers for your reference: \n",
    "<li>https://arxiv.org/abs/1511.02992</li>\n",
    "<li>https://www.sciencedirect.com/science/article/pii/S0893608018300054?via%3Dihub</li>\n",
    "<li>http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "class TrafficSignClassifier(nn.Module):\n",
    "    def __init__(self, num_gpu):\n",
    "        super(TrafficSignClassifier, self).__init__()\n",
    "        self.ngpu = num_gpu\n",
    "       \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, padding=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=(2,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=(2,2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=(2,2))\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, padding=(2,2))\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5, padding=(2,2))\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.linear = nn.Linear(4*4*64, num_classes)\n",
    "        ####\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x  = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = self.linear(x) \n",
    "        return x\n",
    "\t\n",
    "class TrafficSignClassifier1(nn.Module):\n",
    "\tdef __init__(self, num_gpu):\n",
    "\t\tsuper(TrafficSignClassifier1, self).__init__()\n",
    "\t\tself.ngpu = num_gpu\n",
    "\t\t\n",
    "\t\tself.conv0 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1)\n",
    "\t\t\n",
    "\t\tself.conv1a = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding = (1, 1))\n",
    "\t\tself.conv1b = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "\t\t\n",
    "\t\tself.conv2a = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding = (1, 1))\n",
    "\t\tself.conv2b = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding = (1, 1))\n",
    "\t\t\n",
    "\t\tself.conv3a = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding = (1, 1))\n",
    "\t\tself.conv3b = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding = (1, 1))\n",
    "\t\t\n",
    "\t\tself.pool = nn.MaxPool2d(3, 3)\n",
    "\t\t\n",
    "\t\tself.dropout = nn.Dropout2d(p = 0.2)\n",
    "\t\t\n",
    "\t\tself.linear1 = nn.Linear(32*10*10, 128)\n",
    "\t\tself.linear2 = nn.Linear(64*3*3, 128)\n",
    "\t\tself.linear3 = nn.Linear(128, num_classes)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv0(x)\n",
    "\t\tx = self.conv1a(x)\n",
    "\t\tx = self.conv1b(x)\n",
    "\t\tx = self.pool(x)\n",
    "\t\tx1 = self.dropout(x)\n",
    "\n",
    "\t\tx = self.conv2a(x1)\n",
    "\t\tx = self.conv2b(x)\n",
    "\t\tx = self.pool(x)\n",
    "\t\tx2 = self.dropout(x)\n",
    "\t\n",
    "\t\tx = self.conv3a(x2)\n",
    "\t\tx = self.conv3b(x)\n",
    "\t\tx = self.pool(x)\n",
    "\t\tx3 = self.dropout(x)\n",
    "\n",
    "\t\tx1 = x1.view(-1, 32*10*10)\n",
    "\t\tx1 = self.linear1(x1)\n",
    "\t\tx2 = x2.view(-1, 64*3*3)\n",
    "\t\tx2 = self.linear2(x2)\n",
    "\n",
    "\t\tx3 = x3.view(-1, 128*1*1)\n",
    "\t\tx3 = x1 + x2 + x3\n",
    "\t\tx3 = self.linear3(x3)\n",
    "\t\t\n",
    "\t\treturn x3\n",
    "\n",
    "class TrafficSignClassifier2(nn.Module):\n",
    "\tdef __init__(self, num_gpu):\n",
    "\t\tsuper(TrafficSignClassifier2, self).__init__()\n",
    "\t\tself.ngpu = num_gpu\n",
    "\t\t\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=1)\n",
    "\t\t\n",
    "\t\t# self.conv2 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "\t\tself.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding = (1, 1))\n",
    "\t\t\n",
    "\t\tself.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding = (2, 2))\n",
    "\t\t\n",
    "\t\tself.pool4 = nn.MaxPool2d(3, 3)\n",
    "\t\t\n",
    "\t\tself.linear1 = nn.Linear(32*(32*32*3 + 10*10), 128)\n",
    "\t\tself.linear2 = nn.Linear(32*32*32, num_classes)\n",
    "\t\tself.linear4 = nn.Linear(32*10*10, 32*32*32)\n",
    "\t\t\n",
    "\t\tself.dropout = nn.Dropout(p = 0.2)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx1 = self.conv1(x)\n",
    "\t\tx1 = x1.view(32*32*32, -1)\n",
    "\t\t\n",
    "\t\tx2 = self.conv1(x)\n",
    "\t\tx2 = self.conv2(x2)\n",
    "\t\tx2 = x2.view(32*32*32, -1)\n",
    "\t\t\n",
    "\t\tx3 = self.conv1(x)\n",
    "\t\tx3 = self.conv3(x3)\n",
    "\t\tx3 = x3.view(32*32*32, -1)\n",
    "\t\t\n",
    "\t\tx4 = self.conv1(x)\n",
    "\t\tx4 = self.conv2(x4)\n",
    "\t\tx4 = self.pool4(x4)\n",
    "\t\tx4 = x4.view(-1, 32*10*10)\n",
    "\t\tx4 = self.linear4(x4)\n",
    "\t\tx4 = x4.view(32*32*32, -1)\n",
    "\t\t\n",
    "\t\tx = x1 + x2 + x3 + x4\n",
    "\t\t# print(x.size())\n",
    "\t\tx = x.view(-1, 32*32*32)\n",
    "\t\tx = self.linear2(x)\n",
    "\t\t\n",
    "\t\treturn x\n",
    "\t\t\n",
    "\t\t\n",
    "print ('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n  (module): TrafficSignClassifier(\n    (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (conv5): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (conv6): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace)\n    (linear): Linear(in_features=1024, out_features=43, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "### Choose different CNN architectures here! ###\n",
    "\n",
    "classifier = TrafficSignClassifier(num_gpu).to(device)\n",
    "# classifier = TrafficSignClassifier1(num_gpu).to(device)\n",
    "# classifier = TrafficSignClassifier2(num_gpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (num_gpu > 0):\n",
    "    classifier = nn.DataParallel(classifier, list(range(num_gpu)))\n",
    "    \n",
    "#To load the trained weights\n",
    "#Uncomment the following line to load the weights. Then you can run testing directly or continue the training\n",
    "#classfier.load_state_dict(torch.load('./MP2weights.pth', map_location='cpu')) \n",
    "#classfier.eval()\n",
    "\n",
    "#Print the model\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training environment\n",
    "You can choose different loss functions and optimizers. Here I just use the same ones as in PyTorch official tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training parameters\n",
    "##TODO\n",
    "criterion = nn.CrossEntropyLoss()#loss function\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start the training. Just like in tutorial, you can print out your loss and the running time at the end of each epoch to monitor the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_val_accuracy(valloader):\n",
    "\tcorrect = 0.\n",
    "\ttotal = 0.\n",
    "\tpredictions = []\n",
    "\n",
    "\tclass_correct = list(0. for i in range(num_classes))\n",
    "\tclass_total = list(0. for i in range(num_classes))\n",
    "\n",
    "\tfor data in valloader:\n",
    "\t\timages, labels = data\n",
    "\t\tif num_gpu > 0:\n",
    "\t\t\timages = images.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\t\toutputs = classifier(Variable(images))\n",
    "\t\t_, predicted = torch.max(outputs.data, 1)\n",
    "\t\tpredictions.extend(list(predicted.cpu().numpy()))\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum()\n",
    "\n",
    "\t\tc = (predicted == labels).squeeze()\n",
    "\t\tfor i in range(len(labels)):\n",
    "\t\t\tlabel = labels[i]\n",
    "\t\t\tclass_correct[label] += c[i]\n",
    "\t\t\tclass_total[label] += 1\n",
    "\n",
    "\tclass_accuracy = 100.0 * np.divide(class_correct, class_total)\n",
    "\treturn 100.0*correct/total, class_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss =  806.9417221546173  accuracy = 57 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : loss =  399.1725882291794  accuracy = 81 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 : loss =  224.00715658068657  accuracy = 90 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 : loss =  137.96181973814964  accuracy = 95 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 : loss =  94.24093714356422  accuracy = 96 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 : loss =  68.65207313746214  accuracy = 97 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 : loss =  52.89260942488909  accuracy = 98 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 : loss =  42.09600331634283  accuracy = 98 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 : loss =  34.525117833167315  accuracy = 98 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 : loss =  28.97259545698762  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 : loss =  24.709835454821587  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 : loss =  21.412008743733168  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 : loss =  18.699668738991022  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 : loss =  16.325629107654095  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 : loss =  14.554653953760862  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 : loss =  13.034559898078442  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 : loss =  11.806757416576147  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 : loss =  10.517870213836432  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 : loss =  9.561412498354912  accuracy = 99 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 : loss =  8.799490505829453  accuracy = 99 %\nFinished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop, no need to run if you already loaded the weights\n",
    "for epoch in range(20):  # loop over the dataset multiple times  \n",
    "\trunning_loss = 0.0\n",
    "\tfor i, data in enumerate(trainClassifierLoader, 0):\n",
    "\t\t# get the inputs\n",
    "\t\tinputs, labels = data\n",
    "\t\tif num_gpu > 0:\n",
    "\t\t\tinputs = inputs.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\t\n",
    "\t\t# zero out the parameter gradients\n",
    "\t\t#Every time a variable is back propogated through, the gradient will be accumulated instead of being replaced. \n",
    "\t\toptimizer.zero_grad()\n",
    "\t\n",
    "\t\t# forward + backward + optimize\n",
    "\t\toutputs = classifier(inputs)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\n",
    "\t\t#loss.backward() computes dloss/dx for every parameter x\n",
    "\t\tloss.backward()\t\n",
    "\t\n",
    "\t\t#optimizer.step updates the value of x using the gradient x.grad.\n",
    "\t\toptimizer.step() \n",
    "\t\n",
    "\t\t# print statistics\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\t\n",
    "\t\t# if i % 100 == 99:    # print every 2000 mini-batches\n",
    "\t\t# \tprint('[%d, %5d] loss: %.3f' %\n",
    "\t\t# \t\t(epoch + 1, i + 1, running_loss / 2000))\n",
    "\t\t# \trunning_loss = 0.0\n",
    "\t\n",
    "\tacc, class_acc = calculate_val_accuracy(trainClassifierLoader)\n",
    "\tprint('epoch', epoch, ': loss = ', running_loss, ' accuracy = %d %%' % acc)\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "Now our model training is finished. If you are satisfied by the result from part of the test set, let's try it on all testing images. Print out the accuracy on all test images. Note you need to achieve 97% accuracy to get full grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99 %\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_class_acc = calculate_val_accuracy(testClassifierLoader)\n",
    "print('accuracy = %d %%' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Results for Each Individual Class\n",
    "To help futher improve the accuracy, we want to know for which classes our NN works well and for which classes it fails. Print out the accuracy of your classifier on each class on the whole testing dataset. Try to explain why some classess have low accuracy in your report. You don't have to speficy the name of each class. Just use something like \"class 0\" is good enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of     0 : 100 %\nAccuracy of     1 : 100 %\nAccuracy of     2 : 99 %\nAccuracy of     3 : 100 %\nAccuracy of     4 : 100 %\nAccuracy of     5 : 99 %\nAccuracy of     6 : 100 %\nAccuracy of     7 : 100 %\nAccuracy of     8 : 100 %\nAccuracy of     9 : 100 %\nAccuracy of    10 : 100 %\nAccuracy of    11 : 100 %\nAccuracy of    12 : 100 %\nAccuracy of    13 : 100 %\nAccuracy of    14 : 100 %\nAccuracy of    15 : 100 %\nAccuracy of    16 : 100 %\nAccuracy of    17 : 100 %\nAccuracy of    18 : 100 %\nAccuracy of    19 : 100 %\nAccuracy of    20 : 100 %\nAccuracy of    21 : 98 %\nAccuracy of    22 : 100 %\nAccuracy of    23 : 99 %\nAccuracy of    24 : 100 %\nAccuracy of    25 : 100 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 100 %\nAccuracy of    28 : 100 %\nAccuracy of    29 : 100 %\nAccuracy of    30 : 100 %\nAccuracy of    31 : 100 %\nAccuracy of    32 : 100 %\nAccuracy of    33 : 100 %\nAccuracy of    34 : 100 %\nAccuracy of    35 : 100 %\nAccuracy of    36 : 100 %\nAccuracy of    37 : 100 %\nAccuracy of    38 : 100 %\nAccuracy of    39 : 100 %\nAccuracy of    40 : 100 %\nAccuracy of    41 : 100 %\nAccuracy of    42 : 100 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(num_classes))\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "classes = list(range(num_classes))\n",
    "with torch.no_grad():\n",
    "\tfor data in testClassifierLoader:\n",
    "\t\timages, labels = data\n",
    "\t\timages = images.cuda()\n",
    "\t\tlabels = labels.cuda()\n",
    "\t\toutputs = classifier(images)\n",
    "\t\t_, predicted = torch.max(outputs, 1)\n",
    "\t\tc = (predicted == labels).squeeze()\n",
    "\t\tfor i in range(labels.size(0)):\n",
    "\t\t\tlabel = labels[i]\n",
    "\t\t\tclass_correct[label] += c[i].item()\n",
    "\t\t\tclass_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(num_classes):\n",
    "\tprint('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Trained Weights\n",
    "Notice that the trained weights are just variables right now and will be lost when you close the Jupyter file. Obviously, you don't want to train the model again and again. PyTorch can help you save your work. Read the \"Saving & Loading Model for Inference\" part from this tutorial: https://pytorch.org/tutorials/beginner/saving_loading_models.html.<br>\n",
    "Please save the weights from your best model into \"MP2weights.pth\" and include it in your submission. TAs will not train the CNN for you. So if we cannot find this file, you will lose a lot of points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the weights into \"./MP2weights.pth\"\n",
    "torch.save(classifier.state_dict(), \"./cnn_inception.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Advanced Techniques\n",
    "Congratulations! Now we have built a wonderful trffic sign classifier. If you are unsatified about the accuraccy. Here are a few tricks that can help you improve the results:<br>\n",
    "<li>Balance the dataset. Currently, the size of image dataset from different types of traffic signs vary from 100 to 2000. Hence, our CNN will be trained to be very good at classify those types with a lot of image samples, but behave poorly on the rest of types. One possible solution is to generate more image data from the \"unfavorable\" types to make each type to have similar number of image samples. </li>\n",
    "<li>The selection of loss functions and optimizaters  and the learning rate could make a difference. Here is a good reference for different types of loss functions: https://isaacchanghau.github.io/post/loss_functions/</li>\n",
    "<li>Training the model for too long will make our CNN overfit the training set. If that is the case, we could change the number of epoches to avoid it. </li>\n",
    "<li>There are piles of articles online that teaches you how to improve your CNN. Do some Google search yourself and try implement some interesting ones.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break It If You Can!\n",
    "Now if you already have trained a neural network with accuracy higher than 90%, let's try to break our system by manipulating the test set. <br>\n",
    "Find the smallest amount of salt and pepper (recall from lecture 2) noise that has to be added to any image in the data set that was classified correctly, for the image with noise to be miss-classified. Write a function to find this smallest noise for mis-classification, for each image class. Then explore how the smallest noise changes across the different classes. Perform the same experiment with gaussian noise instead of salt and pepper noise. What conclusions can you draw about robustness of your classifier from these experiments? <br>\n",
    "Note: In this part you only need to pick one single image and test your code on it. Please put the original image and the image after adding noises into your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5d52695a911d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# classifier.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_class_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_val_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvtestClassifierLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy = %d %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ef51557bdfb9>\u001b[0m in \u001b[0;36mcalculate_val_accuracy\u001b[0;34m(valloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "adv_test_root_sp = '/home/peixin/mp2_data/test_adv/salt_pepper/images/'\n",
    "adv_test_root_gaussian = '/home/peixin/mp2_data/test_adv/gaussian/images/'\n",
    "adv_test_root = '/home/peixin/mp2_data/test_adv/'\n",
    "img_name = '00000_00027.ppm'\n",
    "# Image to be tested: /home/peixin/mp2_data/Online-Test-sort/00000/00000_00027.ppm\n",
    "\n",
    "# add s&p or gaussian noise to A SINGLE image\n",
    "# read_dir: the directory of the image\n",
    "# write_dir: directory to same the new image\n",
    "# method: 'sp' or 'gaussian'\n",
    "# salt & pepper: arg = probability of noise\n",
    "# gaussian: arg = sigma of gaussian distribution\n",
    "def add_noise(read_dir, write_dir, method, arg):\n",
    "\timg = cv2.imread(read_dir)\n",
    "\trow, col , channel = img.shape\n",
    "\tif method == 'sp':\n",
    "\t\tthres = 1 - arg/2\n",
    "\n",
    "\t\tfor i in range(row):\n",
    "\t\t\tfor j in range(col):\n",
    "\t\t\t\trand = random.random()\n",
    "\t\t\t\t# black\n",
    "\t\t\t\tif rand < arg/2:\n",
    "\t\t\t\t\timg[i, j, :] = [0, 0, 0]\n",
    "\t\t\t\t# white\n",
    "\t\t\t\telif rand > thres:\n",
    "\t\t\t\t\timg[i, j, :] = [255, 255, 255]\n",
    "\telif method == 'gaussian':\n",
    "\t\tnoise = np.random.normal(0, arg, (row, col, channel))\n",
    "\t\tnoise = np.reshape(noise, (row, col, channel))\n",
    "\t\timg = img + noise\n",
    "\t\t\n",
    "\tcv2.imwrite(write_dir, img)\n",
    "\t\n",
    "add_noise(os.path.join(adv_test_root, img_name), os.path.join(adv_test_root_gaussian, 'img.ppm'), 'gaussian', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5d52695a911d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# classifier.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_class_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_val_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvtestClassifierLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy = %d %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ef51557bdfb9>\u001b[0m in \u001b[0;36mcalculate_val_accuracy\u001b[0;34m(valloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# this part does not work yet!\n",
    "# To do:\n",
    "# 1. make a new testing dataset that contains noisy images in 2-3 classes\n",
    "# 2. change the paths below to load the dataset, load the saved model\n",
    "# ()and test it using \n",
    "# calculate_val_accuracy function (defined above)\n",
    "# 3. parameter tuning: find the smallest amount of noise needed to make\n",
    "# the cnn mis-classify the noisy images\n",
    "\n",
    "adv_test_root_sp = '/home/peixin/mp2_data/test_adv/salt_pepper/'\n",
    "adv_test_root_gaussian = '/home/peixin/mp2_data/test_adv/gaussian/'\n",
    "advtestClassifierDataset = dset.ImageFolder(root=adv_test_root_sp,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\t\t\t\t\t\t\t   normImgTensor\n",
    "                           ]))\n",
    "advtestClassifierLoader = torch.utils.data.DataLoader(testClassifierDataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "# calculate accuracy\n",
    "# classifier = torch.load('/home/peixin/mp2/cnn_3layer.pth')\n",
    "# classifier.eval()\n",
    "\n",
    "test_acc, test_class_acc = calculate_val_accuracy(advtestClassifierLoader)\n",
    "print('accuracy = %d %%' % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
